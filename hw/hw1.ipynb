{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, в предобработанной версии не хватает только лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import hw.tg_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/paperspace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/paperspace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/paperspace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(';', ':', ',', '.', '!', '?')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sent_detector.PUNCTUATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. [5 баллов] Классификация текстов\n",
    "Задача классификации формулируется так: данный фрагмент фанфика описывают какую-то ситуацию, которая предшествует произнесению заклинания. Требуется по тексту предсказать, какое именно заклинание будет произнесено. Таким образом, заклинание - это фактически метка класса. Основная мера качества – macro $F_1$.\n",
    "Обучите несколько классификаторов и сравните их между собой. Оцените качество классификаторов на частых и редких классах. Какие классы чаще всего оказываются перепутаны? Связаны ли ошибки со смыслом заклинаний?\n",
    "\n",
    "Используйте фрагменты из множества train для обучения, из множества dev для отладки, из множества test – для тестирования и получения итоговых результатов. \n",
    "\n",
    "1. [1 балл] Используйте fastText в качестве baseline-классификатора.\n",
    "2. [2 балла] Используйте сверточные  или реккурентные сети в качестве более продвинутого классификатора. Поэкспериментируйте с количеством и размерностью фильтров, используйте разные размеры окон, попробуйте использовать $k$-max pooling. \n",
    "3. [2 балла] Попробуйте расширить обучающее множество за счет аугментации данных. Если вам понадобится словарь синонимов, можно использовать WordNet (ниже вы найдете примеры).\n",
    "\n",
    "[бонус] Используйте результат max pooling'а как эмбеддинг входного текста. Визуализируйте эмбеддинги 500-1000 предложений из обучающего множества и изучите свойства получившегося пространства.\n",
    "\n",
    "[бонус] Используйте ваш любимый классификатор и любые (честные) способы повышения качества классификации и получите macro $F_1$ больше 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.vocab import vocab as torchVocab\n",
    "import gensim\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"./hpac_corpus/hpac_training_128.tsv\", sep=\"\\t\", header=None, names=[\"score\", \"spell\", \"sent\"])\n",
    "dev_df = pd.read_csv(\"./hpac_corpus/hpac_dev_128.tsv\", sep=\"\\t\", header=None, names=[\"score\", \"spell\", \"sent\"])\n",
    "test_df = pd.read_csv(\"./hpac_corpus/hpac_test_128.tsv\", sep=\"\\t\", header=None, names=[\"score\", \"spell\", \"sent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.transforms import RegexTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_data(df):\n",
    "    punct = set(list(punctuation) + [\"``\", \"--\", \"n\", \"''\", \"...\"])\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    def preprocess_sent(s):\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        res = []\n",
    "        for tok in tokens:\n",
    "            if tok not in punct:\n",
    "                res.append(tok)\n",
    "        return \" \".join(res)\n",
    "    df[\"preprocess_sent\"] = df[\"sent\"].apply(preprocess_sent)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess_data(train_df)\n",
    "test_df = preprocess_data(test_df)\n",
    "dev_df = preprocess_data(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.fasttext.load_facebook_vectors(\"/home/paperspace/hw-1-nlp-hse-2022-takkat14/hw/word_reprs_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(train_df[\"spell\"].values).union(set(test_df[\"spell\"].values)).union(set(dev_df[\"spell\"].values))\n",
    "label2idx = {label:i for i, label in enumerate(labels)}\n",
    "idx2label = {label2idx[label]:label for label in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "\n",
    "class HPAC_Dataset(IterableDataset):\n",
    "    def __init__(self, df, label_col=\"spell\", sent_col=\"preprocess_sent\", vector_model=w2v, aug=False, aug_prob=0.15, emb_dim=100, max_len=256, label2idx=label2idx):\n",
    "        self.data = df[[label_col, sent_col]]\n",
    "        self.model = vector_model\n",
    "        self.aug = aug\n",
    "        self.max_len = max_len\n",
    "        self.emb_dim = emb_dim \n",
    "        self.label2idx = label2idx     \n",
    "        self.aug_prob = aug_prob  \n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i, sample in self.data.iterrows():\n",
    "            features = np.zeros((self.max_len, self.emb_dim))\n",
    "            for j, word in enumerate(sample[1].split(\" \")):\n",
    "                if j == self.max_len:\n",
    "                    break\n",
    "                if not self.aug:\n",
    "                    features[j] = self.model.get_vector(word)\n",
    "                else:\n",
    "                    if i % 100 == 0 and np.random.rand() > 1 - self.aug_prob:\n",
    "                        choose = np.random.choice(10)\n",
    "                        if np.random.rand() > 0.5:\n",
    "                            features[j] = self.model.get_vector(self.model.most_similar(positive=[word])[choose][0])\n",
    "                        else:\n",
    "                            features[j] = self.model.get_vector(self.model.most_similar(negative=[word])[choose][0])\n",
    "            \n",
    "            yield torch.tensor(features.astype(np.float32)), label2idx[sample[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HPAC_Dataset(train_df)\n",
    "val_dataset = HPAC_Dataset(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, kernel_size=3, padding=1, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(256, 100, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(100, 16, kernel_size=kernel_size, padding=kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1664, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 85)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv1d(256, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(100, 16, kernel_size=(3,), stride=(1,), padding=(3,))\n",
       "    (3): ReLU()\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "    (5): Linear(in_features=1664, out_features=256, bias=True)\n",
       "    (6): ReLU()\n",
       "    (7): Linear(in_features=256, out_features=85, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = CNNModel()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "device = 'cpu'  if torch.cuda.is_available() else torch.cuda.current_device()\n",
    "\n",
    "model.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, pin_memory=True, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, batch_size=BATCH_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_loop(model, opt, tdl, vdl, num_epochs=5, device=device):\n",
    "    mean_train_accs = []\n",
    "    mean_val_accs = []\n",
    "    mean_train_losses = []\n",
    "    mean_val_losses = []\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epocs\"):\n",
    "        model.train()\n",
    "        accs = []\n",
    "        val_accs = []\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        for sample, label in tqdm(tdl, desc=\"Batch TRAIN\"):\n",
    "            opt.step()\n",
    "            sample = sample.to(device)\n",
    "            logits = model(sample)\n",
    "\n",
    "            loss = F.cross_entropy(logits, label)\n",
    "            losses.append(loss.item())\n",
    "            acc = (logits.argmax(axis=1) == label).float().mean().cpu().numpy()\n",
    "            accs.append(acc)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "        \n",
    "        print(f\"Train accuracy: {np.mean(accs)}\")\n",
    "        print(f\"Train loss: {np.mean(losses)}\")\n",
    "        mean_train_accs.append(np.mean(accs))\n",
    "        mean_train_losses.append(np.mean(losses))\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for sample, label in tqdm(vdl, desc=\"Batch VAL\"):\n",
    "            sample = sample.to(device)\n",
    "            logits = model(sample)\n",
    "            loss = F.cross_entropy(logits, label)\n",
    "            val_losses.append(loss.item())\n",
    "            acc = (logits.argmax(axis=1) == label).float().mean().cpu().numpy()\n",
    "            val_accs.append(acc)\n",
    "\n",
    "        print(f\"Val loss: {np.mean(val_losses)}\")\n",
    "        print(f\"Val accuracy {np.mean(val_accs)}\")\n",
    "        mean_val_accs.append(np.mean(val_accs))\n",
    "        mean_val_losses.append(np.mean(val_losses))\n",
    "    return mean_train_accs, mean_val_accs, mean_train_losses, mean_val_losses\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 953it [02:24,  6.59it/s]                                                                                                                                  | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.18932496011257172\n",
      "Train loss: 3.2116410547138385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epocs:  20%|████████████████████████████                                                                                                                | 1/5 [02:38<10:34, 158.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.04071447428535\n",
      "Val accuracy 0.23417681455612183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 953it [02:21,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.2593328654766083\n",
      "Train loss: 2.9063124819292225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epocs:  40%|████████████████████████████████████████████████████████                                                                                    | 2/5 [05:16<07:54, 158.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.9388061591557095\n",
      "Val accuracy 0.2549498677253723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 953it [02:22,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.27953600883483887\n",
      "Train loss: 2.8017337049542292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epocs:  60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 3/5 [07:52<05:14, 157.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.9054960423156997\n",
      "Val accuracy 0.26062312722206116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 953it [02:25,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.2954145073890686\n",
      "Train loss: 2.724819467300383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epocs:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 4/5 [10:32<02:38, 158.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.912301748740573\n",
      "Val accuracy 0.2560547888278961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 953it [02:05,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.30991581082344055\n",
      "Train loss: 2.6478068893628754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epocs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [12:50<00:00, 154.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.9306278108548716\n",
      "Val accuracy 0.2566072344779968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ta, va, tl, vl = train_eval_loop(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HPAC_Dataset(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(test_dataset):\n",
    "    preds = []\n",
    "    gt = []\n",
    "    for s, l in test_dataset:\n",
    "        s.to(device)\n",
    "        pred = model(s.unsqueeze(0)).argmax().item()\n",
    "        preds.append(pred)\n",
    "        gt.append(l)\n",
    "    return gt, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO AUG\n",
      "F1 MICRO: 0.26370621174632114\n",
      "F1 MACRO: 0.06273495812129097\n",
      "ACC: 0.26370621174632114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "gt, preds = get_predictions(test_dataset=test_dataset)\n",
    "print(\"NO AUG\")\n",
    "print(f\"F1 MICRO: {f1_score(gt, preds, average='micro')}\")\n",
    "print(f\"F1 MACRO: {f1_score(gt, preds, average='macro')}\")\n",
    "print(f\"ACC: {accuracy_score(gt, preds)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmetations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HPAC_Dataset(train_df, aug=True)\n",
    "val_dataset = HPAC_Dataset(dev_df, aug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv1d(256, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(100, 16, kernel_size=(3,), stride=(1,), padding=(3,))\n",
       "    (3): ReLU()\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "    (5): Linear(in_features=1664, out_features=256, bias=True)\n",
       "    (6): ReLU()\n",
       "    (7): Linear(in_features=256, out_features=85, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = CNNModel()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "device = 'cpu'  if torch.cuda.is_available() else torch.cuda.current_device()\n",
    "\n",
    "model.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, pin_memory=True, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, batch_size=BATCH_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epocs:   0%|                                                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]/home/paperspace/.local/lib/python3.8/site-packages/gensim/models/fasttext.py:1128: RuntimeWarning: invalid value encountered in divide\n",
      "  return word_vec / np.linalg.norm(word_vec)\n",
      "Batch TRAIN: 953it [03:06,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.12395699322223663\n",
      "Train loss: 3.4683674675219955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch VAL: 119it [00:16,  7.00it/s]\n",
      "Epocs:  20%|████████████████████████████████████████▊                                                                                                                                                                   | 1/5 [03:23<13:35, 203.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.4690754493745435\n",
      "Val accuracy 0.12195774912834167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch TRAIN: 953it [02:57,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.1237274557352066\n",
      "Train loss: 3.448829627862632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch VAL: 119it [00:15,  7.90it/s]\n",
      "Epocs:  40%|█████████████████████████████████████████████████████████████████████████████████▌                                                                                                                          | 2/5 [06:36<09:51, 197.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.4667329307363817\n",
      "Val accuracy 0.12195774912834167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch TRAIN: 953it [02:50,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.12282569706439972\n",
      "Train loss: 3.4474591273200725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch VAL: 119it [00:13,  8.90it/s]\n",
      "Epocs:  60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                 | 3/5 [09:40<06:22, 191.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.4659212637348333\n",
      "Val accuracy 0.12195774912834167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch TRAIN: 953it [02:55,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.12320279330015182\n",
      "Train loss: 3.4464511931380346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch VAL: 119it [00:15,  7.77it/s]\n",
      "Epocs:  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 4/5 [12:51<03:11, 191.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.4654005194912436\n",
      "Val accuracy 0.12195774912834167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch TRAIN: 953it [02:56,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.12325198203325272\n",
      "Train loss: 3.4459221665781667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch VAL: 119it [00:14,  8.19it/s]\n",
      "Epocs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [16:02<00:00, 192.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.4649450538539086\n",
      "Val accuracy 0.12195774912834167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ta, va, tl, vl = train_eval_loop(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HPAC_Dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUG\n",
      "F1 MICRO: 0.11915614012241177\n",
      "F1 MACRO: 0.0025349911898666843\n",
      "ACC: 0.11915614012241177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "gt, preds = get_predictions(test_dataset=test_dataset)\n",
    "print(\"AUG\")\n",
    "print(f\"F1 MICRO: {f1_score(gt, preds, average='micro')}\")\n",
    "print(f\"F1 MACRO: {f1_score(gt, preds, average='macro')}\")\n",
    "print(f\"ACC: {accuracy_score(gt, preds)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не фортануло, что сказать"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
